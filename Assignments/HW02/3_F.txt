The differences in execution times for mmul1, mmul2, and mmul3 stem from how their loop ordering affects memory access patterns and data localization. mmul2 improves cache efficiency by accessing matrix B in a more localized manner, leading to better performance than mmul1, while mmul3 suffers from poor cache reuse due to less localized memory access. mmul1 and mmul4 exhibit nearly identical performance since they use the same loop order, with mmul4 relying on std::vector<double>, which remains contiguous in memory and does not significantly impact execution time. Mmul1 is likely marginally more memory efficient as it uses arrays
